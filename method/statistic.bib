% Encoding: UTF-8

@Article{efron1979,
  author    = {Efron, B.},
  title     = {Bootstrap Methods: Another Look at the Jackknife},
  journal   = {Ann. Statist.},
  year      = {1979},
  volume    = {7},
  number    = {1},
  pages     = {1--26},
  month     = {01},
  doi       = {10.1214/aos/1176344552},
  fjournal  = {The Annals of Statistics},
  groups    = {bootstrap},
  publisher = {The Institute of Mathematical Statistics},
  url       = {https://doi.org/10.1214/aos/1176344552},
}

@InCollection{Jolliffe2011,
  author    = {Jolliffe, Ian},
  title     = {Principal Component Analysis},
  booktitle = {International Encyclopedia of Statistical Science},
  publisher = {Springer},
  year      = {2011},
  month     = jan,
  isbn      = {978-3-642-04897-5},
  date      = {2011-01-01},
  doi       = {10.1007/978-3-642-04898-2_455},
  groups    = {PCA},
  url       = {http://dx.doi.org/10.1007/978-3-642-04898-2_455},
}

@Article{doi:10.1002/wics.101,
  author   = {Abdi, Hervé and Williams, Lynne J.},
  title    = {Principal component analysis},
  journal  = {Wiley Interdisciplinary Reviews: Computational Statistics},
  volume   = {2},
  number   = {4},
  pages    = {433-459},
  abstract = {Abstract Principal component analysis (PCA) is a multivariate technique that analyzes a data table in which observations are described by several inter-correlated quantitative dependent variables. Its goal is to extract the important information from the table, to represent it as a set of new orthogonal variables called principal components, and to display the pattern of similarity of the observations and of the variables as points in maps. The quality of the PCA model can be evaluated using cross-validation techniques such as the bootstrap and the jackknife. PCA can be generalized as correspondence analysis (CA) in order to handle qualitative variables and as multiple factor analysis (MFA) in order to handle heterogeneous sets of variables. Mathematically, PCA depends upon the eigen-decomposition of positive semi-definite matrices and upon the singular value decomposition (SVD) of rectangular matrices. Copyright © 2010 John Wiley \& Sons, Inc. This article is categorized under: Statistical and Graphical Methods of Data Analysis > Multivariate Analysis Statistical and Graphical Methods of Data Analysis > Dimension Reduction},
  doi      = {10.1002/wics.101},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/wics.101},
  groups   = {PCA},
  keywords = {singular and eigen value decomposition, bilinear decomposition, factor scores and loadings, RESS PRESS, multiple factor analysis},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wics.101},
}

@Article{WOLD198737,
  author   = {Svante Wold and Kim Esbensen and Paul Geladi},
  title    = {Principal component analysis},
  journal  = {Chemometrics and Intelligent Laboratory Systems},
  year     = {1987},
  volume   = {2},
  number   = {1},
  pages    = {37 - 52},
  issn     = {0169-7439},
  note     = {Proceedings of the Multivariate Statistical Workshop for Geologists and Geochemists},
  abstract = {Principal component analysis of a data matrix extracts the dominant patterns in the matrix in terms of a complementary set of score and loading plots. It is the responsibility of the data analyst to formulate the scientific issue at hand in terms of PC projections, PLS regressions, etc. Ask yourself, or the investigator, why the data matrix was collected, and for what purpose the experiments and measurements were made. Specify before the analysis what kinds of patterns you would expect and what you would find exciting. The results of the analysis depend on the scaling of the matrix, which therefore must be specified. Variance scaling, where each variable is scaled to unit variance, can be recommended for general use, provided that almost constant variables are left unscaled. Combining different types of variables warrants blockscaling. In the initial analysis, look for outliers and strong groupings in the plots, indicating that the data matrix perhaps should be “polished” or whether disjoint modeling is the proper course. For plotting purposes, two or three principal components are usually sufficient, but for modeling purposes the number of significant components should be properly determined, e.g. by cross-validation. Use the resulting principal components to guide your continued investigation or chemical experimentation, not as an end in itself.},
  doi      = {https://doi.org/10.1016/0169-7439(87)80084-9},
  url      = {http://www.sciencedirect.com/science/article/pii/0169743987800849},
}

@Book{,
  title     = {Bayesian Inference in Statistical Analysis},
  publisher = {wiley},
  year      = {2011},
  author    = {George E.P. Box, George C. Tiao},
  isbn      = {9781118033197 ,
9780471574286},
  doi       = {10.1002/9781118033197},
  groups    = {Bayesian},
}

@InCollection{Dempster2008,
  author    = {Dempster, Arthur P.},
  title     = {A Generalization of Bayesian Inference},
  booktitle = {Classic Works of the Dempster-Shafer Theory of Belief Functions},
  publisher = {Springer},
  year      = {2008},
  month     = jan,
  isbn      = {978-3-540-25381-5},
  abstract  = {Procedures of statistical inference are described which generalize Bayesian inference in specific ways. Probability is used in such a way that in general only bounds may be placed on the probabilities of given events, and probability systems of this kind are suggested both for sample information and for prior information. These systems are then combined using a specified rule. Illustrations are given for inferences about trinomial probabilities, and for inferences about a monotone sequence of binomial pi. Finally, some comments are made on the general class of models which produce upper and lower probabilities, and on the specific models which underlie the suggested inference procedures.},
  date      = {2008-01-01},
  doi       = {10.1007/978-3-540-44792-4_4},
  groups    = {Bayesian},
  url       = {http://dx.doi.org/10.1007/978-3-540-44792-4_4},
}

@Book{,
  title     = {Machine Learning: A Probabilistic Perspective},
  publisher = {MIT Press},
  year      = {2012},
  author    = {Kevin P. Murphy},
  groups    = {probability},
}

@Article{Kingma2013,
  author        = {Kingma, D. P. and Welling, M.},
  title         = {Auto-Encoding Variational Bayes},
  journal       = {ArXiv e-prints},
  year          = {2013},
  month         = dec,
  __markedentry = {[ling:]},
  abstract      = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archiveprefix = {arXiv},
  eprint        = {1312.6114},
  groups        = {VAE},
  keywords      = {Statistics - Machine Learning, Computer Science - Learning},
  primaryclass  = {stat.ML},
  url           = {http://adsabs.harvard.edu/abs/2013arXiv1312.6114K},
}

@Article{Doersch2016,
  author        = {Doersch, C.},
  title         = {Tutorial on Variational Autoencoders},
  journal       = {ArXiv e-prints},
  year          = {2016},
  month         = jun,
  __markedentry = {[ling:6]},
  abstract      = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
  archiveprefix = {arXiv},
  eprint        = {1606.05908},
  groups        = {VAE},
  keywords      = {Statistics - Machine Learning, Computer Science - Learning},
  primaryclass  = {stat.ML},
  url           = {http://adsabs.harvard.edu/abs/2016arXiv160605908D},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:random\;0\;1\;\;\;\;;
2 StaticGroup:bootstrap\;0\;1\;\;\;\;;
1 StaticGroup:statistic\;0\;1\;\;\;\;;
2 StaticGroup:PCA\;0\;1\;\;\;\;;
2 StaticGroup:VAE\;0\;1\;\;\;\;;
1 StaticGroup:Bayesian\;0\;1\;\;\;\;;
1 StaticGroup:probability\;0\;1\;\;\;\;;
}
