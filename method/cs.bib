% Encoding: UTF-8

@Article{Doersch2016,
  author        = {Doersch, C.},
  title         = {Tutorial on Variational Autoencoders},
  journal       = {ArXiv e-prints},
  year          = {2016},
  month         = jun,
  abstract      = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
  archiveprefix = {arXiv},
  eprint        = {1606.05908},
  groups        = {VAE},
  keywords      = {Statistics - Machine Learning, Computer Science - Learning},
  primaryclass  = {stat.ML},
  url           = {http://adsabs.harvard.edu/abs/2016arXiv160605908D},
}

@Article{Kingma2013,
  author        = {Kingma, D. P. and Welling, M.},
  title         = {Auto-Encoding Variational Bayes},
  journal       = {ArXiv e-prints},
  year          = {2013},
  month         = dec,
  abstract      = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archiveprefix = {arXiv},
  eprint        = {1312.6114},
  groups        = {VAE},
  keywords      = {Statistics - Machine Learning, Computer Science - Learning},
  primaryclass  = {stat.ML},
  url           = {http://adsabs.harvard.edu/abs/2013arXiv1312.6114K},
}

@Article{Noroozi2016,
  author        = {Noroozi, M. and Favaro, P.},
  title         = {Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles},
  journal       = {ArXiv e-prints},
  year          = {2016},
  month         = mar,
  abstract      = {In this paper we study the problem of image representation learning without human annotation. By following the principles of self-supervision, we build a convolutional neural network (CNN) that can be trained to solve Jigsaw puzzles as a pretext task, which requires no manual labeling, and then later repurposed to solve object classification and detection. To maintain the compatibility across tasks we introduce the context-free network (CFN), a siamese-ennead CNN. The CFN takes image tiles as input and explicitly limits the receptive field (or context) of its early processing units to one tile at a time. We show that the CFN includes fewer parameters than AlexNet while preserving the same semantic learning capabilities. By training the CFN to solve Jigsaw puzzles, we learn both a feature mapping of object parts as well as their correct spatial arrangement. Our experimental evaluations show that the learned features capture semantically relevant content. Our proposed method for learning visual representations outperforms state of the art methods in several transfer learning benchmarks.},
  archiveprefix = {arXiv},
  eprint        = {1603.09246},
  groups        = {Visual Representations},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition},
  primaryclass  = {cs.CV},
  url           = {http://adsabs.harvard.edu/abs/2016arXiv160309246N},
}

@Article{Walt2011,
  author   = {S. van der Walt and S. C. Colbert and G. Varoquaux},
  title    = {The NumPy Array: A Structure for Efficient Numerical Computation},
  journal  = {Computing in Science Engineering},
  year     = {2011},
  volume   = {13},
  number   = {2},
  pages    = {22--30},
  month    = mar,
  issn     = {1521-9615},
  doi      = {10.1109/MCSE.2011.37},
  groups   = {python},
  keywords = {data structures, high level languages, mathematics computing, numerical analysis, numerical computation, numpy array, numerical data, high level language, Python programming language, Arrays, Numerical analysis, Performance evaluation, Computational efficiency, Finite element methods, Vector quantization, Resource management, Python, NumPy, scientific programming, numerical computations, programming libraries},
}

@Article{Hunter2007,
  author   = {J. D. Hunter},
  title    = {Matplotlib: A {2D} Graphics Environment},
  journal  = {Computing in Science Engineering},
  year     = {2007},
  volume   = {9},
  number   = {3},
  pages    = {90--95},
  month    = may,
  issn     = {1521-9615},
  doi      = {10.1109/MCSE.2007.55},
  groups   = {python},
  keywords = {computer graphics, mathematics computing, object-oriented programming, software packages, Matplotlib, 2D graphics package, Python, application development, interactive scripting, publication-quality image generation, user interface, operating system, Graphics, Interpolation, Equations, Graphical user interfaces, Packaging, Image generation, User interfaces, Operating systems, Computer languages, Programming profession, Python, scripting languages, application development, scientific programming},
}

@Article{Perez2007,
  author        = {F. Perez and B. E. Granger},
  title         = {IPython: A System for Interactive Scientific Computing},
  journal       = {Computing in Science Engineering},
  year          = {2007},
  volume        = {9},
  number        = {3},
  pages         = {21--29},
  month         = may,
  issn          = {1521-9615},
  __markedentry = {[ling:]},
  doi           = {10.1109/MCSE.2007.53},
  groups        = {python},
  keywords      = {data visualisation, natural sciences computing, object-oriented languages, object-oriented programming, parallel programming, software libraries, IPython, interactive scientific computing, comprehensive library, data visualization, distributed computation, parallel computation, Scientific computing, Libraries, Data visualization, Spine, Supercomputers, Hardware, Data analysis, Testing, Production, Parallel processing, Python, computer languages, scientific programming, scientific computing},
}

@Article{Oliphant2007,
  author        = {T. E. Oliphant},
  title         = {Python for Scientific Computing},
  journal       = {Computing in Science Engineering},
  year          = {2007},
  volume        = {9},
  number        = {3},
  pages         = {10--20},
  month         = may,
  issn          = {1521-9615},
  __markedentry = {[ling:6]},
  doi           = {10.1109/MCSE.2007.58},
  groups        = {python},
  keywords      = {high level languages, Python, scientific computing, steering language, scientific codes, high-level language, Scientific computing, High level languages, Libraries, Writing, Application software, Embedded software, Software standards, Standards development, Internet, Prototypes, Python, computer languages, scientific programming, scientific computing},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:ML\;0\;1\;\;\;\;;
2 StaticGroup:VAE\;0\;1\;\;\;\;;
2 StaticGroup:Visual Representations\;0\;1\;\;\;\;;
1 StaticGroup:DL\;0\;1\;\;\;\;;
1 StaticGroup:python\;0\;1\;\;\;\;;
}
